{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from context_cite.context_partitioner import HybridPartitioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"context.txt\", \"r\") as f:\n",
    "\tsample_context = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_response = \"The authors used P100 GPUs in their experiments.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_partitioner = HybridPartitioner(\n",
    "    context=sample_context,\n",
    "    response=sample_response\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_partitioner.split_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Paragraphs that passed the BM25 and semantic filtering:\n",
      "Paragraph 1: Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms are used in conjunction with a recurrent network.\n",
      "\n",
      "Paragraph 2: In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
      "\n",
      "Paragraph 3: On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is listed in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.\n",
      "\n",
      "Paragraph 4: Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature. We estimate the number of floating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each GPU.\n",
      "\n",
      "Paragraph 5: development set, newstest2013. We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table 3.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_paragraphs = hybrid_partitioner.get_filtered_paragraphs()\n",
    "print(\"Filtered Paragraphs that passed the BM25 and semantic filtering:\")\n",
    "for idx, para in enumerate(filtered_paragraphs, 1):\n",
    "    print(f\"Paragraph {idx}: {para}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sources: 12\n",
      "Final Sources:\n",
      "Source 1: Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 19].\n",
      "\n",
      "Source 2: In all but a few cases [27], however, such attention mechanisms are used in conjunction with a recurrent network.\n",
      "\n",
      "Source 3: In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output.\n",
      "\n",
      "Source 4: The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
      "\n",
      "Source 5: On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4.\n",
      "\n",
      "Source 6: The configuration of this model is listed in the bottom line of Table 3.\n",
      "\n",
      "Source 7: Training took 3.5 days on 8 P100 GPUs.\n",
      "\n",
      "Source 8: Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.\n",
      "\n",
      "Source 9: Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature.\n",
      "\n",
      "Source 10: We estimate the number of floating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each GPU. development set, newstest2013.\n",
      "\n",
      "Source 11: We used beam search as described in the previous section, but no checkpoint averaging.\n",
      "\n",
      "Source 12: We present these results in Table 3.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of sources:\", hybrid_partitioner.num_sources)\n",
    "print(\"Final Sources:\")\n",
    "for i, source in enumerate(hybrid_partitioner.parts):\n",
    "    print(f\"Source {i+1}: {source}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
