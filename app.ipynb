{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing libraries...\n",
      "Importing complete\n",
      "Loading modules\n",
      "Initializing LLM model...\n",
      "LLM model initialized!\n",
      "Modules loaded successfully\n",
      "Creating Gradio interface...\n",
      "Gradio interface created successfully\n",
      "Launching demo...\n",
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://ba2649488ff675ebef.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://ba2649488ff675ebef.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\W_Envi\\Anaconda\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "e:\\W_Envi\\Anaconda\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attributed: The Transformer is a simple neural network architecture proposed by the authors, which is based solely on attention mechanisms, rather than recurrence or convolutional neural networks. It is a model that connects the encoder and decoder through an attention mechanism, allowing for global dependencies to be modeled between input and output sequences. This architecture is designed to be more parallelizable and efficient than traditional models, making it suitable for large-scale training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eec494bfb1704e469bf1faf9fa0f79ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\W_Envi\\Anaconda\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "e:\\W_Envi\\Anaconda\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attributed: The Transformer is a simple neural network architecture proposed by the authors, which is based solely on attention mechanisms, rather than recurrence or convolutional neural networks. It is a model that connects the encoder and decoder through an attention mechanism, allowing for global dependencies to be modeled between input and output sequences. This architecture is designed to be more parallelizable and efficient than traditional models, making it suitable for large-scale training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47bf051586e342d4adaa233db3e18271",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Importing libraries...\")\n",
    "import gradio as gr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from llama_cloud_services import LlamaParse\n",
    "from dotenv import load_dotenv\n",
    "from context_cite import ContextCiter\n",
    "from context_cite.utils import aggregate_logit_probs\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Import transformers here so we can load the model once\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "print(\"Importing complete\")\n",
    "print(\"Loading modules\")\n",
    "load_dotenv()\n",
    "parser = LlamaParse(api_key=os.getenv(\"LLAMA_CLOUD_API_TOKEN\"))\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 1) Load the LLM model/weights ONCE at startup\n",
    "# ----------------------------------------------------------\n",
    "print(\"Initializing LLM model...\")\n",
    "model_name_or_path = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
    "model.to(device)\n",
    "\n",
    "# Tokenizer setup\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "tokenizer.padding_side = \"left\"\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"LLM model initialized!\")\n",
    "\n",
    "\n",
    "def plot(cc: ContextCiter) -> plt.Figure:\n",
    "    pred_logs = cc._logit_probs\n",
    "    pred_logits = aggregate_logit_probs(pred_logs)\n",
    "    actu_logits = cc._actual_logit_probs\n",
    "\n",
    "    preds = pred_logits.flatten()\n",
    "    actus = actu_logits.flatten()\n",
    "    assert len(preds) == len(actus), f\"{len(preds)} != {len(actus)}\"\n",
    "\n",
    "    # Compute Spearman correlation\n",
    "    corr, _ = spearmanr(preds, actus)\n",
    "\n",
    "    # Create figure explicitly\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    plt.scatter(preds, actus, alpha=0.3, label=\"Context ablations\")\n",
    "\n",
    "    # Plot reference line\n",
    "    x_line = np.linspace(min(preds.min(), actus.min()), \n",
    "                        max(preds.max(), actus.max()), 100)\n",
    "    plt.plot(x_line, x_line, '--', color='gray', label=\"y = x\")\n",
    "\n",
    "    # Add labels and styling\n",
    "    plt.xlabel(\"Predicted log-probability\")\n",
    "    plt.ylabel(\"Actual log-probability\")\n",
    "    plt.title(f\"Predicted vs. Actual log-probability\\nSpearman correlation: {corr:.2f}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Close plot to prevent memory leaks and return figure\n",
    "    plt.close()\n",
    "    return fig\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 2) Define core function, reusing the already-loaded model\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "def analyze_document(file, query, top_k=5, num_ablations=64):\n",
    "    # Process input file\n",
    "    if hasattr(file, 'name') and file.name.endswith(\".pdf\"):\n",
    "        docs = parser.load_data(file.name)\n",
    "        context = \" \".join([doc.text for doc in docs if len(doc.text) >= 32])\n",
    "    elif hasattr(file, 'name') and file.name.endswith(\".txt\"):\n",
    "        with open(file.name, \"r\", encoding=\"utf-8\") as f:\n",
    "            context = f.read()\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Please upload .pdf or .txt.\")\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # Create ContextCiter, but use the preloaded model and tokenizer\n",
    "    # ------------------------------------------------------\n",
    "    cc = ContextCiter(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        context=context,\n",
    "        query=query,\n",
    "        num_ablations=num_ablations,\n",
    "    )\n",
    "\n",
    "    # Get results\n",
    "    df = cc.get_attributions(as_dataframe=True, top_k=top_k)\n",
    "\n",
    "    # Create plot\n",
    "    fig = plot(cc)\n",
    "\n",
    "    return df, fig\n",
    "\n",
    "print(\"Modules loaded successfully\")\n",
    "print(\"Creating Gradio interface...\")\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=analyze_document,\n",
    "    inputs=[\n",
    "        gr.File(label=\"Upload Document (PDF or TXT)\"),\n",
    "        gr.Textbox(label=\"Query\", value=\"What is Transformer?\"),\n",
    "        gr.Slider(1, 10, value=5, label=\"Top K Results\"),\n",
    "        gr.Slider(16, 128, value=64, label=\"Number of Ablations\")\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Dataframe(label=\"Attribution Results\"),\n",
    "        gr.Plot(label=\"Probability Correlation\")\n",
    "    ],\n",
    "    title=\"ContextCite Document Analysis Demo\",\n",
    "    description=\"Upload a document and ask questions about its content\"\n",
    ")\n",
    "print(\"Gradio interface created successfully\")\n",
    "print(\"Launching demo...\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
